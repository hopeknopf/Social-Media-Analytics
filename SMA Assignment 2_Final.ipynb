{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMA Assignment 2\n",
    "## Group Members: Cherry, Sagar, Amy, Hope, Cory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib\n",
    "import http\n",
    "import pandas as pd\n",
    "from copy import copy\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "def get_comment_and_link(x, driver):\n",
    "    try:\n",
    "        link = x.find_element_by_tag_name('a').get_attribute('href')\n",
    "        hover = ActionChains(driver).move_to_element(x)\n",
    "        hover.perform()\n",
    "        time.sleep(1)\n",
    "        comment_count = x.find_elements_by_class_name('-V_eO')[1].find_element_by_tag_name('span').text\n",
    "        return (link, comment_count)\n",
    "    except IndexError:\n",
    "        return (None, None)\n",
    "    \n",
    "def scrape_links_and_comments(n_scrape=0):\n",
    "    n_scrape = n_scrape\n",
    "    driver = webdriver.Chrome('/Users/Cory/Desktop/chromedriver')  # Optional argument, if not specified will search path.\n",
    "    driver.get('https://www.instagram.com');\n",
    "    \n",
    "    # Get to Instagram and click the login box\n",
    "    time.sleep(1) # Let the user actually see something!\n",
    "    login = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/article/div[2]/div[2]/p/a')\n",
    "    login.click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Log in - Insert your username and password below\n",
    "    user_box = driver.find_element_by_name('username')\n",
    "    user_box.send_keys('username')\n",
    "    pass_box = driver.find_element_by_name('password')\n",
    "    pass_box.send_keys('password')\n",
    "    time.sleep(1)\n",
    "    pass_box.submit()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # How to actually scrape\n",
    "    target = 'natgeo'\n",
    "    driver.get('https://www.instagram.com/' + target +'/')\n",
    "    \n",
    "    # Scrolling up and down\n",
    "    n_scrape2 = copy(n_scrape) - 12\n",
    "    series_of_links = []\n",
    "    while n_scrape2 > 0:\n",
    "        series_of_pictures = None\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # One scroll down catches 12 images\n",
    "        time.sleep(5)\n",
    "        series_of_pictures = pd.Series(driver.find_elements_by_css_selector('div.v1Nh3.kIKUG._bz0w')[:-12]).apply(get_comment_and_link, args=(driver,))\n",
    "        shortlist_links = series_of_pictures.tolist()\n",
    "        for i in shortlist_links:\n",
    "            series_of_links.append(i)\n",
    "        n_scrape2 -= 12\n",
    "        time.sleep(1)\n",
    "    links = set(series_of_links)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = scrape_links_and_comments(1000)\n",
    "df = pd.DataFrame(a)\n",
    "\n",
    "df.to_csv(\"~/Downloads/insta-1.csv\") # This can be fed ahead into Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the scraper I ran earlier, we already have the comment counts and CSS selector links for our instagram posts\n",
    "\n",
    "df = pd.read_csv('~/Downloads/insta-1.csv')\n",
    "df = df.dropna(how = 'any', axis = 0)\n",
    "df = df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1)\n",
    "\n",
    "pics = []\n",
    "likes = []\n",
    "captions = []\n",
    "\n",
    "links = df['0'].tolist()\n",
    "\n",
    "driver = webdriver.Chrome('/Users/Cory/Desktop/chromedriver')  # Optional argument, if not specified will search path.\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    driver.get(link)\n",
    "    try:\n",
    "        picture = driver.find_element_by_class_name('KL4Bh').find_element_by_tag_name('img').get_attribute('src')\n",
    "        pics.append(picture)\n",
    "        \n",
    "        like_count = driver.find_element_by_class_name('zV_Nj').find_element_by_tag_name('span').text\n",
    "        likes.append(like_count)\n",
    "        \n",
    "        caption = driver.find_element_by_class_name('C4VMK').find_element_by_tag_name('span').text\n",
    "        captions.append(caption)\n",
    "\n",
    "        time.sleep(2)\n",
    "    except NoSuchElementException:\n",
    "        pics.append(None)\n",
    "        likes.append(None)\n",
    "        captions.append(None)\n",
    "        time.sleep(1)\n",
    "        \n",
    "df['pic_link'] = pics\n",
    "df['likes'] = likes\n",
    "df['caption'] = captions\n",
    "df = df.rename({'0':'link','1':'comments'}, axis=1)\n",
    "df = df.dropna(how = 'any', axis = 0)\n",
    "df.to_csv(\"~/Downloads/natgeo-insta-final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "from google.cloud.vision import types\n",
    "from google.cloud import vision\n",
    "import numpy as np\n",
    "import urllib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calling the API using key\n",
    "client = vision.ImageAnnotatorClient.from_service_account_file('C://Users//cherr//Downloads//SocialMedia-48509d145dcb.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the data\n",
    "data = pd.read_csv('insta-1.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the data\n",
    "data = data.rename(index=str, columns={0: \"index\", 1: \"URL\"})\n",
    "data.dropna(inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_tags(url):\n",
    "    resp = urllib.urlopen(url)\n",
    "    content = resp.read()\n",
    "    image_tags = []\n",
    "    image = vision.types.Image(content=content)\n",
    "    response = client.label_detection(image=image)\n",
    "    labels = response.label_annotations\n",
    "    for label in labels:\n",
    "        image_tags.append(label.description)\n",
    "    return(image_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['google_image_tags'] = data['URL'].map(image_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('image_tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>link</th>\n",
       "      <th>comments</th>\n",
       "      <th>pic_link</th>\n",
       "      <th>likes</th>\n",
       "      <th>caption</th>\n",
       "      <th>google_image_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.instagram.com/p/Bs7tibqDpOT/</td>\n",
       "      <td>1,160</td>\n",
       "      <td>https://scontent-dfw5-1.cdninstagram.com/vp/ba...</td>\n",
       "      <td>240,900</td>\n",
       "      <td>Photo by @noralorek | Peter Mandela, a 12-year...</td>\n",
       "      <td>[u'Brown', u'Standing', u'Tree', u'Fun', u'Foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.instagram.com/p/BrKZ6ttg0W7/</td>\n",
       "      <td>1,125</td>\n",
       "      <td>https://scontent-dfw5-1.cdninstagram.com/vp/17...</td>\n",
       "      <td>318,788</td>\n",
       "      <td>Photo by @BrianSkerry | Researcher Enric Sala ...</td>\n",
       "      <td>[u'Rock', u'Earth', u'Circle', u'World']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                                      link  \\\n",
       "0           0             0  https://www.instagram.com/p/Bs7tibqDpOT/   \n",
       "1           1             1  https://www.instagram.com/p/BrKZ6ttg0W7/   \n",
       "\n",
       "  comments                                           pic_link    likes  \\\n",
       "0    1,160  https://scontent-dfw5-1.cdninstagram.com/vp/ba...  240,900   \n",
       "1    1,125  https://scontent-dfw5-1.cdninstagram.com/vp/17...  318,788   \n",
       "\n",
       "                                             caption  \\\n",
       "0  Photo by @noralorek | Peter Mandela, a 12-year...   \n",
       "1  Photo by @BrianSkerry | Researcher Enric Sala ...   \n",
       "\n",
       "                                   google_image_tags  \n",
       "0  [u'Brown', u'Standing', u'Tree', u'Fun', u'Foo...  \n",
       "1           [u'Rock', u'Earth', u'Circle', u'World']  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('image_tags.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(874, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the dataframe, remove unnecessary columns\n",
    "df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>comments</th>\n",
       "      <th>pic_link</th>\n",
       "      <th>likes</th>\n",
       "      <th>caption</th>\n",
       "      <th>google_image_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.instagram.com/p/Bs7tibqDpOT/</td>\n",
       "      <td>1,160</td>\n",
       "      <td>https://scontent-dfw5-1.cdninstagram.com/vp/ba...</td>\n",
       "      <td>240,900</td>\n",
       "      <td>Photo by @noralorek | Peter Mandela, a 12-year...</td>\n",
       "      <td>[Brown, Standing, Tree, Fun, Footwear, Grass, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.instagram.com/p/BrKZ6ttg0W7/</td>\n",
       "      <td>1,125</td>\n",
       "      <td>https://scontent-dfw5-1.cdninstagram.com/vp/17...</td>\n",
       "      <td>318,788</td>\n",
       "      <td>Photo by @BrianSkerry | Researcher Enric Sala ...</td>\n",
       "      <td>[Rock, Earth, Circle, World]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       link comments  \\\n",
       "0  https://www.instagram.com/p/Bs7tibqDpOT/    1,160   \n",
       "1  https://www.instagram.com/p/BrKZ6ttg0W7/    1,125   \n",
       "\n",
       "                                            pic_link    likes  \\\n",
       "0  https://scontent-dfw5-1.cdninstagram.com/vp/ba...  240,900   \n",
       "1  https://scontent-dfw5-1.cdninstagram.com/vp/17...  318,788   \n",
       "\n",
       "                                             caption  \\\n",
       "0  Photo by @noralorek | Peter Mandela, a 12-year...   \n",
       "1  Photo by @BrianSkerry | Researcher Enric Sala ...   \n",
       "\n",
       "                                   google_image_tags  \n",
       "0  [Brown, Standing, Tree, Fun, Footwear, Grass, ...  \n",
       "1                       [Rock, Earth, Circle, World]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the list is read as string in the google_image_tags column, lets correct this\n",
    "from ast import literal_eval\n",
    "df['google_image_tags'] = df['google_image_tags'].map(lambda x: literal_eval(x))\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>comments</th>\n",
       "      <th>pic_link</th>\n",
       "      <th>likes</th>\n",
       "      <th>caption</th>\n",
       "      <th>google_image_tags</th>\n",
       "      <th>google_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.instagram.com/p/Bs7tibqDpOT/</td>\n",
       "      <td>1,160</td>\n",
       "      <td>https://scontent-dfw5-1.cdninstagram.com/vp/ba...</td>\n",
       "      <td>240,900</td>\n",
       "      <td>Photo by @noralorek | Peter Mandela, a 12-year...</td>\n",
       "      <td>[Brown, Standing, Tree, Fun, Footwear, Grass, ...</td>\n",
       "      <td>Brown Standing Tree Fun Footwear Grass Walking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.instagram.com/p/BrKZ6ttg0W7/</td>\n",
       "      <td>1,125</td>\n",
       "      <td>https://scontent-dfw5-1.cdninstagram.com/vp/17...</td>\n",
       "      <td>318,788</td>\n",
       "      <td>Photo by @BrianSkerry | Researcher Enric Sala ...</td>\n",
       "      <td>[Rock, Earth, Circle, World]</td>\n",
       "      <td>Rock Earth Circle World</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       link comments  \\\n",
       "0  https://www.instagram.com/p/Bs7tibqDpOT/    1,160   \n",
       "1  https://www.instagram.com/p/BrKZ6ttg0W7/    1,125   \n",
       "\n",
       "                                            pic_link    likes  \\\n",
       "0  https://scontent-dfw5-1.cdninstagram.com/vp/ba...  240,900   \n",
       "1  https://scontent-dfw5-1.cdninstagram.com/vp/17...  318,788   \n",
       "\n",
       "                                             caption  \\\n",
       "0  Photo by @noralorek | Peter Mandela, a 12-year...   \n",
       "1  Photo by @BrianSkerry | Researcher Enric Sala ...   \n",
       "\n",
       "                                   google_image_tags  \\\n",
       "0  [Brown, Standing, Tree, Fun, Footwear, Grass, ...   \n",
       "1                       [Rock, Earth, Circle, World]   \n",
       "\n",
       "                                       google_labels  \n",
       "0  Brown Standing Tree Fun Footwear Grass Walking...  \n",
       "1                            Rock Earth Circle World  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the list of words into a continuous string\n",
    "df['google_labels'] = df['google_image_tags'].map(lambda x: \" \".join(x))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>caption</th>\n",
       "      <th>google_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.instagram.com/p/Bs7tibqDpOT/</td>\n",
       "      <td>240,900</td>\n",
       "      <td>1,160</td>\n",
       "      <td>Photo by @noralorek | Peter Mandela, a 12-year...</td>\n",
       "      <td>Brown Standing Tree Fun Footwear Grass Walking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.instagram.com/p/BrKZ6ttg0W7/</td>\n",
       "      <td>318,788</td>\n",
       "      <td>1,125</td>\n",
       "      <td>Photo by @BrianSkerry | Researcher Enric Sala ...</td>\n",
       "      <td>Rock Earth Circle World</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       link    likes comments  \\\n",
       "0  https://www.instagram.com/p/Bs7tibqDpOT/  240,900    1,160   \n",
       "1  https://www.instagram.com/p/BrKZ6ttg0W7/  318,788    1,125   \n",
       "\n",
       "                                             caption  \\\n",
       "0  Photo by @noralorek | Peter Mandela, a 12-year...   \n",
       "1  Photo by @BrianSkerry | Researcher Enric Sala ...   \n",
       "\n",
       "                                       google_labels  \n",
       "0  Brown Standing Tree Fun Footwear Grass Walking...  \n",
       "1                            Rock Earth Circle World  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a new column order and remove pic_link\n",
    "column_order = ['link', 'likes', 'comments', 'caption', 'google_labels']\n",
    "df2 = df.loc[:, column_order]\n",
    "\n",
    "df2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the caption column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the part of the text before the '|' character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def remove_char(s):\n",
    "    s = re.sub(\".*\\|\\s\", \"\", s)\n",
    "    return s\n",
    "\n",
    "df2['caption_new'] = df2['caption'].map(lambda x: remove_char(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# lemmatization\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lemm = WordNetLemmatizer()\n",
    "\n",
    "def convert_to_valid_pos(x):\n",
    "    \n",
    "    x = x[0].upper() # extract first character of the POS tag\n",
    "    \n",
    "    # define mapping for the tag to correct tag.\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "               \"N\": wordnet.NOUN,\n",
    "               \"R\": wordnet.ADV,\n",
    "               \"V\": wordnet.VERB}\n",
    "    \n",
    "    return tag_dict.get(x, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(s):\n",
    "    pos_tagged_text = nltk.pos_tag(word_tokenize(s.lower()))\n",
    "    \n",
    "    lemm_list = []\n",
    "\n",
    "    for (word, tag) in pos_tagged_text:\n",
    "        lemm_list.append(word_lemm.lemmatize(word, pos = convert_to_valid_pos(tag)))\n",
    "\n",
    "\n",
    "    lemm_text = \" \".join(lemm_list)\n",
    "    return lemm_text\n",
    "\n",
    "df2['caption_new'] = df2['caption_new'].map(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove anomalous strings from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords # store english stopwords in a list\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "def remove_anomalies(s):\n",
    "    \n",
    "    tokens = word_tokenize(s)\n",
    "    \n",
    "    # urls\n",
    "    weblinks = [w for w in tokens if \".co.uk\" in w] + [w for w in tokens if \".com\" in w] + [w for w in tokens if \"www\" in w]\n",
    "    weblinks = list(set(weblinks)) # remove duplicates from weblinks\n",
    "    \n",
    "    # numbers\n",
    "    numbers = []\n",
    "    for x in tokens:\n",
    "        if len(re.findall('.*[0-9]+.*', x)) > 0:\n",
    "            numbers.append(re.findall('.*[0-9]+.*', x)[0])\n",
    "        else:\n",
    "            numbers.append(np.nan)\n",
    "    \n",
    "    numbers = pd.Series(numbers)\n",
    "    numbers = numbers[~numbers.isnull()].tolist()\n",
    "    \n",
    "    # stars\n",
    "    stars = []\n",
    "    for x in tokens:\n",
    "        if len(re.findall('.*[\\*]+.*', x)) > 0:\n",
    "            stars.append(re.findall('.*[\\*]+,*', x)[0])\n",
    "        else:\n",
    "            stars.append(0)\n",
    "        \n",
    "    stars = pd.Series(stars)   \n",
    "    stars = stars[stars != 0].tolist()\n",
    "    \n",
    "    #stopwords\n",
    "    global en_stopwords\n",
    "    \n",
    "    answer = \" \".join([w for w in tokens if (w not in en_stopwords) & (w not in weblinks) & (w not in numbers) & (w not in stars)])\n",
    "    \n",
    "    for l in punctuation:\n",
    "        answer = answer.replace(l, \"\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "df2['caption_new'] = df2['caption_new'].map(remove_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new final dataframe that will be used for the analyses ahead.\n",
    "df_final = df2.loc[:, ['link', 'google_labels', 'caption_new', 'likes', 'comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct likes to remove commas and convert to integers\n",
    "df_final['likes'] = df_final['likes'].map(lambda x: float(re.sub(\",\",\"\",x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove commas from the comments column\n",
    "df_final['comments'] = df_final['comments'].map(lambda x: re.sub(\",\", \"\", str(x)))\n",
    "\n",
    "# some comment rows have values such as 13k etc. we need to correct these\n",
    "def correct_k_values(s):\n",
    "    if \"k\" in s:\n",
    "        s = re.sub(\"k\", \"\", s)\n",
    "        s = str(float(s)*1000)\n",
    "        return s\n",
    "    else:\n",
    "        return s\n",
    "    \n",
    "df_final['comments'] = df_final['comments'].map(lambda x: correct_k_values(x))\n",
    "\n",
    "# convert to numbers\n",
    "df_final['comments'] = df_final['comments'].map(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>google_labels</th>\n",
       "      <th>caption_new</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>caption_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.instagram.com/p/Bs7tibqDpOT/</td>\n",
       "      <td>Brown Standing Tree Fun Footwear Grass Walking...</td>\n",
       "      <td>peter mandela  refugee south sudan living ugan...</td>\n",
       "      <td>240900.0</td>\n",
       "      <td>1160.0</td>\n",
       "      <td>peter mandela  refugee south sudan living ugan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.instagram.com/p/BrKZ6ttg0W7/</td>\n",
       "      <td>Rock Earth Circle World</td>\n",
       "      <td>researcher enric sala hovers giant  circularsh...</td>\n",
       "      <td>318788.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>researcher enric sala hovers giant  circularsh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.instagram.com/p/BpjB91kDCdP/</td>\n",
       "      <td>Natural landscape Nature Natural environment T...</td>\n",
       "      <td>first assignment  natgeo eleven year ago  take...</td>\n",
       "      <td>341539.0</td>\n",
       "      <td>658.0</td>\n",
       "      <td>first assignment  natgeo eleven year ago  take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.instagram.com/p/BqscPhdh2M4/</td>\n",
       "      <td>Electronic instrument Recording Musician Broad...</td>\n",
       "      <td>part commitment enhance learn education techno...</td>\n",
       "      <td>100086.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>part commitment enhance learn education techno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.instagram.com/p/BrYxkCxAq16/</td>\n",
       "      <td>Green Cactus Plant Grass Prickly pear Aerial p...</td>\n",
       "      <td>believe flamingos withstand boil water tempera...</td>\n",
       "      <td>440059.0</td>\n",
       "      <td>1577.0</td>\n",
       "      <td>believe flamingos withstand boil water tempera...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       link  \\\n",
       "0  https://www.instagram.com/p/Bs7tibqDpOT/   \n",
       "1  https://www.instagram.com/p/BrKZ6ttg0W7/   \n",
       "2  https://www.instagram.com/p/BpjB91kDCdP/   \n",
       "3  https://www.instagram.com/p/BqscPhdh2M4/   \n",
       "4  https://www.instagram.com/p/BrYxkCxAq16/   \n",
       "\n",
       "                                       google_labels  \\\n",
       "0  Brown Standing Tree Fun Footwear Grass Walking...   \n",
       "1                            Rock Earth Circle World   \n",
       "2  Natural landscape Nature Natural environment T...   \n",
       "3  Electronic instrument Recording Musician Broad...   \n",
       "4  Green Cactus Plant Grass Prickly pear Aerial p...   \n",
       "\n",
       "                                         caption_new     likes  comments  \\\n",
       "0  peter mandela  refugee south sudan living ugan...  240900.0    1160.0   \n",
       "1  researcher enric sala hovers giant  circularsh...  318788.0    1125.0   \n",
       "2  first assignment  natgeo eleven year ago  take...  341539.0     658.0   \n",
       "3  part commitment enhance learn education techno...  100086.0     270.0   \n",
       "4  believe flamingos withstand boil water tempera...  440059.0    1577.0   \n",
       "\n",
       "                                      caption_labels  \n",
       "0  peter mandela  refugee south sudan living ugan...  \n",
       "1  researcher enric sala hovers giant  circularsh...  \n",
       "2  first assignment  natgeo eleven year ago  take...  \n",
       "3  part commitment enhance learn education techno...  \n",
       "4  believe flamingos withstand boil water tempera...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a new column that takes both captions and labels\n",
    "# we need to combine the captions and labels into a single column\n",
    "df_final['caption_labels'] = df_final['caption_new'] + \" \" + df_final['google_labels']\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using just google labels\n",
    "First we just need to use the google labels to predict engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the comments and likes\n",
    "Engagement is the weighted score of comments and likes and is defined as 0.6 * comments + 0.4 * likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "# standardize the likes and comments\n",
    "df_final_std = std_scaler.fit_transform(df_final.loc[:, ['likes', 'comments']])\n",
    "engagement_score = df_final_std[:, 0]*0.4 + df_final_std[:, 1]*0.6\n",
    "\n",
    "# in case we want a dataframe back\n",
    "feature_names = ['likes', 'comments', 'engagement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_like_comment_engg = pd.DataFrame(np.c_[df_final_std, engagement_score], columns = ['likes', 'comments', 'engagement'])\n",
    "\n",
    "df_final_final = pd.concat([df_final.drop(['likes', 'comments'], axis = 1), df_like_comment_engg], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the labels into bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.2841905074541063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Calculate the frequencies of words using the TfidfTransformer\n",
    "X = np.array(df_final.loc[:, 'google_labels'])\n",
    "y = engagement_score\n",
    "\n",
    "# split into test and train data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Convert the arrays into a presence/absence matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features = 200)\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Fit the model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(n_estimators = 50, random_state = 42, max_features = 1)\n",
    "rf_model.fit(X_train_counts, y_train)\n",
    "\n",
    "predicted = rf_model.predict(X_train_counts)\n",
    "\n",
    "# print the accuracies\n",
    "print(\"Train RMSE:\", np.sqrt(mean_squared_error(y_train, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.4s remaining:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "# cross validation scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_scores_rf = cross_val_score(rf_model, \n",
    "                                      X_train_counts,\n",
    "                                      y_train, \n",
    "                                      cv = 5,\n",
    "                                      verbose = 2,\n",
    "                                      n_jobs = -1,\n",
    "                                      scoring = \"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.564885262955436"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*np.mean(cross_val_scores_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.8888911613936642\n"
     ]
    }
   ],
   "source": [
    "# test error\n",
    "predicted = rf_model.predict(X_test_counts)\n",
    "\n",
    "# print the accuracies\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model using just captions\n",
    "Next let's use the captions to predict engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.2597829648368051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Calculate the frequencies of words using the TfidfTransformer\n",
    "X = np.array(df_final.loc[:, 'caption_new'])\n",
    "y = engagement_score\n",
    "\n",
    "# split into test and train data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Convert the arrays into a presence/absence matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features = 200)\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Fit the model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(n_estimators = 100, random_state = 42, max_features = 1)\n",
    "rf_model.fit(X_train_counts, y_train)\n",
    "\n",
    "predicted = rf_model.predict(X_train_counts)\n",
    "\n",
    "# print the accuracies\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "# cross validation scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_scores_rf = cross_val_score(rf_model, \n",
    "                                      X_train_counts,\n",
    "                                      y_train, \n",
    "                                      cv = 5,\n",
    "                                      verbose = 2,\n",
    "                                      n_jobs = -1,\n",
    "                                      scoring = \"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5895979105826803"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*np.mean(cross_val_scores_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 1.0055770994487159\n"
     ]
    }
   ],
   "source": [
    "# test error\n",
    "predicted = rf_model.predict(X_test_counts)\n",
    "\n",
    "# print the accuracies\n",
    "print(\"Train RMSE:\", np.sqrt(mean_squared_error(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that using just google labels leads to a better accuracy than using captions of the images that the photographers have provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model using both captions and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.2529394733443292\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Calculate the frequencies of words using the TfidfTransformer\n",
    "X = np.array(df_final.loc[:, 'caption_labels'])\n",
    "y = engagement_score\n",
    "\n",
    "# split into test and train data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Convert the arrays into a presence/absence matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features = 200)\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Fit the model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(n_estimators = 100, random_state = 42, max_features = 4)\n",
    "rf_model.fit(X_train_counts, y_train)\n",
    "\n",
    "predicted = rf_model.predict(X_train_counts)\n",
    "\n",
    "# print the accuracies\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.2s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "# cross validation scores\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_scores_rf = cross_val_score(rf_model, \n",
    "                                      X_train_counts,\n",
    "                                      y_train, \n",
    "                                      cv = 5,\n",
    "                                      verbose = 2,\n",
    "                                      n_jobs = -1,\n",
    "                                      scoring = \"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5562468050831829"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1*np.mean(cross_val_scores_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.9809533488836648\n"
     ]
    }
   ],
   "source": [
    "# test error\n",
    "predicted = rf_model.predict(X_test_counts)\n",
    "\n",
    "# print the accuracies\n",
    "print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we are better off using just google labels for the prediction task. The RMSE is the least for that case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform topic modeling (LDA) on the original image labels. Choose an appropriate number of topics. You may want to start with 5 topics, but adjust the number up or down depending on the word distributions you get. Decide on a suitable name for each topic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, lda, nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer,RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=df_final['google_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide the number of latent topics to be estimated: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 874\n",
      "INFO:lda:vocab_size: 1289\n",
      "INFO:lda:n_words: 9865\n",
      "INFO:lda:n_topics: 5\n",
      "INFO:lda:n_iter: 500\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -90848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(874, 1289)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:<10> log likelihood: -66927\n",
      "INFO:lda:<20> log likelihood: -64765\n",
      "INFO:lda:<30> log likelihood: -63967\n",
      "INFO:lda:<40> log likelihood: -63474\n",
      "INFO:lda:<50> log likelihood: -63180\n",
      "INFO:lda:<60> log likelihood: -62969\n",
      "INFO:lda:<70> log likelihood: -63003\n",
      "INFO:lda:<80> log likelihood: -62916\n",
      "INFO:lda:<90> log likelihood: -62575\n",
      "INFO:lda:<100> log likelihood: -62578\n",
      "INFO:lda:<110> log likelihood: -62571\n",
      "INFO:lda:<120> log likelihood: -62482\n",
      "INFO:lda:<130> log likelihood: -62472\n",
      "INFO:lda:<140> log likelihood: -62429\n",
      "INFO:lda:<150> log likelihood: -62280\n",
      "INFO:lda:<160> log likelihood: -62255\n",
      "INFO:lda:<170> log likelihood: -62195\n",
      "INFO:lda:<180> log likelihood: -62155\n",
      "INFO:lda:<190> log likelihood: -62164\n",
      "INFO:lda:<200> log likelihood: -61974\n",
      "INFO:lda:<210> log likelihood: -61780\n",
      "INFO:lda:<220> log likelihood: -61573\n",
      "INFO:lda:<230> log likelihood: -61612\n",
      "INFO:lda:<240> log likelihood: -61550\n",
      "INFO:lda:<250> log likelihood: -61545\n",
      "INFO:lda:<260> log likelihood: -61471\n",
      "INFO:lda:<270> log likelihood: -61221\n",
      "INFO:lda:<280> log likelihood: -61207\n",
      "INFO:lda:<290> log likelihood: -61084\n",
      "INFO:lda:<300> log likelihood: -61077\n",
      "INFO:lda:<310> log likelihood: -61132\n",
      "INFO:lda:<320> log likelihood: -61107\n",
      "INFO:lda:<330> log likelihood: -61066\n",
      "INFO:lda:<340> log likelihood: -61062\n",
      "INFO:lda:<350> log likelihood: -60948\n",
      "INFO:lda:<360> log likelihood: -60917\n",
      "INFO:lda:<370> log likelihood: -60912\n",
      "INFO:lda:<380> log likelihood: -60874\n",
      "INFO:lda:<390> log likelihood: -60863\n",
      "INFO:lda:<400> log likelihood: -60917\n",
      "INFO:lda:<410> log likelihood: -60900\n",
      "INFO:lda:<420> log likelihood: -60834\n",
      "INFO:lda:<430> log likelihood: -60914\n",
      "INFO:lda:<440> log likelihood: -60893\n",
      "INFO:lda:<450> log likelihood: -60985\n",
      "INFO:lda:<460> log likelihood: -60839\n",
      "INFO:lda:<470> log likelihood: -60833\n",
      "INFO:lda:<480> log likelihood: -60816\n",
      "INFO:lda:<490> log likelihood: -60878\n",
      "INFO:lda:<499> log likelihood: -60831\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "ntopics= input('Provide the number of latent topics to be estimated: ')\n",
    "\n",
    "def tokenize_text(version_desc):\n",
    "    lowercase=version_desc.lower()\n",
    "    text = wordnet_lemmatizer.lemmatize(lowercase)\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "word_tokenizer=RegexpTokenizer(r'\\w+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords_nltk=set(stopwords.words('english'))\n",
    "\n",
    "vec_words = CountVectorizer(tokenizer=tokenize_text,stop_words=stopwords_nltk,decode_error='ignore')\n",
    "total_features_words = vec_words.fit_transform(labels)\n",
    "\n",
    "print(total_features_words.shape)\n",
    "\n",
    "model = lda.LDA(n_topics=int(ntopics), n_iter=500, random_state=1, alpha = 0.5, eta = 0.005)\n",
    "model.fit(total_features_words)\n",
    "\n",
    "topic_word = model.topic_word_ \n",
    "doc_topic=model.doc_topic_\n",
    "doc_topic=pd.DataFrame(doc_topic)\n",
    "topic_df=df_final.join(doc_topic)\n",
    "natgeo_lda=pd.DataFrame()\n",
    "\n",
    "for i in range(int(ntopics)):\n",
    "    topic=\"topic_\"+str(i)\n",
    "    natgeo_lda[topic]=topic_df.groupby(topic_df[\"caption_new\"])[i].mean()\n",
    "\n",
    "natgeo_lda=natgeo_lda.reset_index()\n",
    "topics=pd.DataFrame(topic_word)\n",
    "topics.columns=vec_words.get_feature_names()\n",
    "topics1=topics.transpose()\n",
    "topics1.to_excel(\"topic_word_dist.xlsx\")\n",
    "natgeo_lda.to_excel(\"natgeo_topic_dist.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_topics = df_final_final.merge(natgeo_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_topics.to_excel(\"final_df_topics.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now sort the data from high to low engagement score, and take the highest and the lowest quartiles (by engagement score). What are the main differences in the average topic weights of images across the two quartiles (e.g., greater weight of some topics in the highest versus lowest engagement quartiles)? Show the main results in a table. \n",
    "\n",
    "This was done in an Excel spreadsheet and the results are summarized in the attached Word Doc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task D - See Submitted Word Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What advice would you give Natgeo to increase engagement on its Instagram page based on your findings in Tasks B and C?   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
